{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:00:04.875884Z",
     "start_time": "2019-07-22T07:59:58.496703Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:18:02.520299Z",
     "start_time": "2019-07-22T08:18:02.515311Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_ag_news(split='train'):\n",
    "    data = pd.read_csv('data/ag_news_csv/{}.csv'.format(split),header=-1)\n",
    "    labels = data[0].values.tolist()\n",
    "    texts = data[1]+data[2]\n",
    "    texts = texts.values.tolist()\n",
    "    return texts,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:18:04.063194Z",
     "start_time": "2019-07-22T08:18:03.373038Z"
    }
   },
   "outputs": [],
   "source": [
    "texts,labels = load_ag_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:18:04.865046Z",
     "start_time": "2019-07-22T08:18:04.853080Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(texts):\n",
    "    #只保留字母和数字空格\n",
    "    texts = [re.sub(r'[^A-Za-z0-9 ]','',text) for text in texts]\n",
    "    #去空格字母小写\n",
    "    texts = [[word.strip().lower() for word in text.split(' ')] for text in texts]\n",
    "    #去停用词\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    texts = [list(filter(lambda x:x not in english_stopwords,text)) for text in texts]\n",
    "    #Stemming\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    texts = [[stemmer.stem(word) for word in text] for text in texts]\n",
    "    #Lemma\n",
    "    lemma = WordNetLemmatizer()\n",
    "    texts = [[lemma.lemmatize(word) for word in text] for text in texts]\n",
    "    texts = [' '.join(text) for text in texts]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:19:36.823034Z",
     "start_time": "2019-07-22T08:18:05.659919Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wall st bear claw back black reutersreut  shortsel wall street dwindlingband ultracyn see green'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts=text_preprocessing(texts)\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:19:37.330647Z",
     "start_time": "2019-07-22T08:19:37.324662Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_label_for_model(labels):\n",
    "    onehot_labels = np_utils.to_categorical(labels)\n",
    "    onehot_labels = onehot_labels[:,1:]\n",
    "    return onehot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:19:37.852254Z",
     "start_time": "2019-07-22T08:19:37.837323Z"
    }
   },
   "outputs": [],
   "source": [
    "onehot_labels = preprocess_label_for_model(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:19:38.400783Z",
     "start_time": "2019-07-22T08:19:38.354907Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train,x_val,y_train,y_val = train_test_split(texts,onehot_labels,test_size=0.001,shuffle=True,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:19:39.410086Z",
     "start_time": "2019-07-22T08:19:39.406094Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text_for_model(x,token=None,dict_size=8000,max_length=50):\n",
    "    if token is None:\n",
    "        token = Tokenizer(num_words=dict_size)\n",
    "        token.fit_on_texts(x)\n",
    "    texts_seq = token.texts_to_sequences(x)\n",
    "    texts_seq_pad = sequence.pad_sequences(texts_seq,maxlen=max_length)\n",
    "    return texts_seq_pad,token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:19:50.231222Z",
     "start_time": "2019-07-22T08:19:40.417391Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train_seq,token = preprocess_text_for_model(x_train)\n",
    "x_val_seq,_ = preprocess_text_for_model(x_val,token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:19:52.227788Z",
     "start_time": "2019-07-22T08:19:52.223801Z"
    }
   },
   "outputs": [],
   "source": [
    "dimension_output=4\n",
    "dict_size=8000\n",
    "embedded_size=128\n",
    "maxlen=50\n",
    "pooling_size=5\n",
    "num_layers=2\n",
    "size_layer=128\n",
    "num_heads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:21:00.373153Z",
     "start_time": "2019-07-22T08:21:00.369164Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:20:19.941026Z",
     "start_time": "2019-07-22T08:20:19.926066Z"
    }
   },
   "outputs": [],
   "source": [
    "def layer_norm(inputs,epsilon=1e-8):\n",
    "    mean,var = tf.nn.moments(inputs,[-1],keep_dims=True) #以一条数据（一个embedding word）为单位进行normalization (10,50,1)\n",
    "    norm = (inputs - mean)/(tf.sqrt(var+epsilon))\n",
    "    params_shape = inputs.get_shape()[-1:] #128\n",
    "    #防止layer normalization破坏学出的数据分布，通过gamma，beta两个学习的参数恢复norm之前的分布\n",
    "    gamma = tf.get_variable('gamma', params_shape, tf.float32, tf.ones_initializer())\n",
    "    beta = tf.get_variable('beta', params_shape, tf.float32, tf.zeros_initializer())\n",
    "    return gamma*norm+beta\n",
    "\n",
    "def multihead_attention(inputs,masks):\n",
    "    '''\n",
    "    把WQ，WK，WV三个矩阵拼起来直接计算，矩阵的第一个维度要和embedding_size相同，第二个维度为embedding乘以矩阵后所得向量的维度（自定义）\n",
    "    embedding乘以一个WQ，WK，WV沿Axis=0方向拼接矩阵，同时得到q,k,v三个向量\n",
    "    '''\n",
    "    Q_K_V = tf.layers.dense(inputs,3*embedded_size,activation=tf.nn.relu) #(10,50,128) dot (128,384) = (10,50,384)\n",
    "    Q,K,V = tf.split(Q_K_V,3,-1) #(10,50,128)\n",
    "    Q_ = tf.concat(tf.split(Q,num_heads,axis=2),axis=0)#(80,50,16)\n",
    "    K_ = tf.concat(tf.split(K,num_heads,axis=2),axis=0)#(80,50,16)\n",
    "    V_ = tf.concat(tf.split(V,num_heads,axis=2),axis=0)#(80,50,16)\n",
    "    weight_bf_softmax = tf.matmul(Q_,tf.transpose(K_,[0,2,1]))/np.sqrt(K_.get_shape().as_list()[-1]) #(80,50,50)  根号里的是k向量的长度\n",
    "    #padding mask因为输入的句子要做padding，不够长的要补0，对于这些0不应该给予attention，通过赋负无穷，使得在softmax过后，权重非常小\n",
    "    if masks is not None:\n",
    "        paddings = tf.fill(tf.shape(weight_bf_softmax),float('-inf'))\n",
    "        weight_bf_softmax = tf.where(tf.equal(masks,0),paddings,weight_bf_softmax)\n",
    "    weight = tf.nn.softmax(weight_bf_softmax) #(80,50,50)\n",
    "    outputs = tf.matmul(weight,V_)  #(80,50,50) dot (80,50,16) = (80,50,16)\n",
    "    outputs = tf.concat(tf.split(outputs,num_heads,0),2) # (10,50,128)\n",
    "    outputs += inputs #残差连接\n",
    "    return layer_norm(outputs)\n",
    "\n",
    "def window_mask(size):\n",
    "    masks = np.zeros([maxlen,maxlen])\n",
    "    for i in range(maxlen):\n",
    "        if i < size:\n",
    "            masks[i,:i+size+1]=1.\n",
    "        elif i > maxlen - size - 1:\n",
    "            masks[i,i-size:]=1.\n",
    "        else:\n",
    "            masks[i,i-size:i+size+1]=1.\n",
    "    masks = tf.convert_to_tensor(masks) #(50,50)\n",
    "    #(1,50,50)  -> (batch_size*heads,50,50),\n",
    "    return tf.tile(tf.expand_dims(masks,0),[tf.shape(x)[0]*num_heads,1,1]) \n",
    "\n",
    "def position_encoding(inputs,embed_dim):\n",
    "    T = inputs.get_shape().as_list()[1] #50\n",
    "    v = tf.range(T) #(50,)\n",
    "    m = tf.expand_dims(v,0) #(1,50)\n",
    "    m = tf.tile(m,[tf.shape(inputs)[0],1]) #(batch_size,50)\n",
    "    lookup_table = tf.get_variable('lookup_table', dtype=tf.float32, shape=[T, embed_dim]) #(50,128)\n",
    "    outputs = tf.nn.embedding_lookup(lookup_table, m) #(batch_size,50,128)\n",
    "    return outputs\n",
    "\n",
    "def pointwise_feedforward(inputs,num_units=[None,None],activation=None):\n",
    "    outputs = tf.layers.conv1d(inputs,num_units[0],kernel_size=1,activation=activation) #(batch_size,50,512)\n",
    "    outputs = tf.layers.conv1d(outputs,num_units[1],kernel_size=1) #(batch_size,50,128)\n",
    "    outputs += inputs #(batch_size,50,128)\n",
    "    return layer_norm(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:21:07.526132Z",
     "start_time": "2019-07-22T08:21:06.594502Z"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.int32,shape=[None,maxlen],name='x')\n",
    "y = tf.placeholder(tf.float32,shape=[None,dimension_output],name='y')\n",
    "with tf.name_scope('embedding'):\n",
    "    embedding_matrix = tf.Variable(tf.random_normal([dict_size,embedded_size],-1,1))\n",
    "    feed = tf.nn.embedding_lookup(embedding_matrix,x)\n",
    "with tf.name_scope('multihead_attention'):\n",
    "    for window_size in range(1,6):\n",
    "        with tf.variable_scope('mask_window_%d' % window_size):\n",
    "            feed = multihead_attention(feed,window_mask(window_size))\n",
    "    feed = tf.add(feed,position_encoding(feed,embedded_size)) #加上position encoding\n",
    "    with tf.variable_scope('multihead'):\n",
    "        feed = multihead_attention(feed, None)#（batch_size,50,128）\n",
    "with tf.name_scope('feed_forward'):\n",
    "    with tf.variable_scope('pointwise'):\n",
    "        feed = pointwise_feedforward(feed, num_units=[4*embedded_size, \n",
    "                                                          embedded_size], activation=tf.nn.relu)\n",
    "with tf.name_scope('output'):\n",
    "    logits = tf.layers.dense(feed,dimension_output)[:,-1]\n",
    "    pred = tf.argmax(logits,1)\n",
    "with tf.name_scope('train'):\n",
    "    loss_func = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss_func)\n",
    "with tf.name_scope('eval'):\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits,1),tf.argmax(y,1)),tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T08:21:11.762391Z",
     "start_time": "2019-07-22T08:21:09.248509Z"
    }
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T04:56:10.561599Z",
     "start_time": "2019-07-22T04:56:10.558606Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs=10\n",
    "batch_size=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T04:56:11.065262Z",
     "start_time": "2019-07-22T04:56:11.060304Z"
    }
   },
   "outputs": [],
   "source": [
    "def shuffle_batch(x,y,batch_size):\n",
    "    index = np.random.permutation(len(x))\n",
    "    n_batch = len(x) // batch_size\n",
    "    for batch_index in (np.array_split(index,n_batch)):\n",
    "        x_batch,y_batch = x[batch_index],y[batch_index]\n",
    "        yield x_batch,y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T05:31:30.069354Z",
     "start_time": "2019-07-22T04:56:12.098634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:00 loss:0.42 acc:0.88\n",
      "epoch:01 loss:0.30 acc:0.89\n",
      "epoch:02 loss:0.30 acc:0.90\n",
      "epoch:03 loss:0.38 acc:0.88\n",
      "epoch:04 loss:0.32 acc:0.88\n",
      "epoch:05 loss:0.30 acc:0.92\n",
      "epoch:06 loss:0.29 acc:0.93\n",
      "epoch:07 loss:0.33 acc:0.90\n",
      "epoch:08 loss:0.31 acc:0.89\n",
      "epoch:09 loss:0.29 acc:0.93\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for x_batch,y_batch in shuffle_batch(x_train_seq,y_train,batch_size):\n",
    "        sess.run(optimizer,feed_dict={x:x_batch,y:y_batch})\n",
    "    l,a = sess.run([loss_func,accuracy],feed_dict={x:x_val_seq,y:y_val})\n",
    "    print('epoch:%02d loss:%.2f acc:%.2f' %(epoch,l,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T05:36:20.775885Z",
     "start_time": "2019-07-22T05:36:20.630737Z"
    }
   },
   "outputs": [],
   "source": [
    "sents_test,labels_test=load_ag_news('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T05:36:36.414880Z",
     "start_time": "2019-07-22T05:36:30.581370Z"
    }
   },
   "outputs": [],
   "source": [
    "texts_test=text_preprocessing(sents_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T05:36:48.217304Z",
     "start_time": "2019-07-22T05:36:47.875057Z"
    }
   },
   "outputs": [],
   "source": [
    "onehot_labels_test = preprocess_label_for_model(labels_test)\n",
    "x_test_seq,_=preprocess_text_for_model(sents_test,token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T05:37:04.139452Z",
     "start_time": "2019-07-22T05:36:58.724196Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred=[]\n",
    "for batch in np.array_split(np.arange(x_test_seq.shape[0]),100):\n",
    "    p = sess.run(pred,feed_dict={x:x_test_seq[batch]})\n",
    "    y_pred.extend(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T05:37:33.344228Z",
     "start_time": "2019-07-22T05:37:32.988683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8063157894736842"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result = [item+1 for item in y_pred]\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(labels_test,final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T05:40:17.827867Z",
     "start_time": "2019-07-22T05:40:17.823876Z"
    }
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
